[{"authors":["admin"],"categories":null,"content":"I am a final year undergraduate student with a major in Electronics \u0026amp; Electrical Engineering and minor in Computer Science \u0026amp; Engineering at IIT Guwahati. I work on problems at the intersection of reinforcement learning (RL) and imitation learning, finding new approaches and methods to combine both and solve challenging Robotics problems. Following my research interests, I did both summer research internships in 2018 and 2019 in the field of Deep RL and Imitation Learning, where I worked under Prof. Balaraman Ravindran at RISE Labs, IIT Madras and earlier with Dr. Venkat Natarajan at Intel Labs, Bengaluru.\nAim for the stars - summarises my free time I spend with my telescope. Sports has remained an integral part of my life, especially Speed Skating. In my 9 years of roller skating career, I stood out as a two times National Champion \u0026amp; five times consecutive State champion.\nI also love trekking, cycling trips \u0026amp; going to unknown places. Although I don\u0026rsquo;t do it that often anymore, please do ping me if you got some excellent trekking plans or suggestions. :)\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1570143917,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vjg28.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a final year undergraduate student with a major in Electronics \u0026amp; Electrical Engineering and minor in Computer Science \u0026amp; Engineering at IIT Guwahati. I work on problems at the intersection of reinforcement learning (RL) and imitation learning, finding new approaches and methods to combine both and solve challenging Robotics problems. Following my research interests, I did both summer research internships in 2018 and 2019 in the field of Deep RL and Imitation Learning, where I worked under Prof.","tags":null,"title":"Varshil Gandhi","type":"authors"},{"authors":[],"categories":[],"content":" IRL Experiment on Mountain Car Code Application\nOverview Inverse reinforcement learning deals with the case of learning the reward function for a situation or an activity where the optimum behavior is known.    Environment Details     Mountain car env : A car (agent) learning how to get to the top of the cliff in the least timesteps. The goal of the agent is to reach the goal position , represented by the flag at position = 0.5 units. The state space is a vector [position, velocity] with  1.2 ≤ position ≤ 0.6 velocity∈[-0.07, 0.07] The above implies that the state space is continuous and infinite.    The agent can perform 3 actions 0 ,1 ,2 :  0: accelerate in the left 1: zero acceleration 2: accelerate in the right  Acceleration = 0.001 units The reward function inbuilt the environment is $R(s,a) = -1$ for each timestep and 0 if it reaches the goal.  Experiment Details NG- Abbel paper vs Ours (Similarities)  A linear function approximator for reward function with 26 basis functions.  A penalty function with penalty constant = 2 in the updates of linear programming.  Equally spaced Gaussian functions as their reward basis functions.  Reward function depend only on the ‘position’ feature of state.   Differences:  Test in the paper was a naive approach (Algo 2). While ours in Algo 3.  The paper discretized the state space $s= [position,~ velocity]$ into $120*120$ discrete states. This makes the number of states finite. \n They created a model based on the discretization of state space. We didn\u0026rsquo;t.  They evaluated the Linear programming maximization for a bunch of 5000 states.   Results  To be added  Questions? Ask\nDocumentation\n","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"1f04687ef48f1868f328b6c4738363f6","permalink":"https://vjg28.github.io/slides/irl-slides/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/slides/irl-slides/","section":"slides","summary":"Documentation of IRL project on Mountain Car","tags":[],"title":"Inverse RL on Mountain Car Slides","type":"slides"},{"authors":[],"categories":[],"content":"","date":1570135511,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570143917,"objectID":"923789c98eab9417512154cd36ca2c42","permalink":"https://vjg28.github.io/project/device-for-ship-detection/","publishdate":"2019-10-04T02:15:11+05:30","relpermalink":"/project/device-for-ship-detection/","section":"project","summary":"A working prototype of a low cost safety device for fishing vessels to prevent collisions.","tags":["Others"],"title":"Device for Ship Detection","type":"project"},{"authors":[],"categories":[],"content":"","date":1570135238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570143917,"objectID":"e519847ef7fd09b6dc1dd9d66d1d50a9","permalink":"https://vjg28.github.io/project/bert-on-narrativeqa/","publishdate":"2019-10-04T02:10:38+05:30","relpermalink":"/project/bert-on-narrativeqa/","section":"project","summary":"A transformer based Language model BERT trained on NarrativeQA dataset. Part of a bigger ongoing project.","tags":["NLP"],"title":"BERT on NarrativeQA","type":"project"},{"authors":[],"categories":[],"content":"","date":1570134796,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570143917,"objectID":"27023f036a36fd43bcee521a3bf155a7","permalink":"https://vjg28.github.io/project/fake-news-detector/","publishdate":"2019-10-04T02:03:16+05:30","relpermalink":"/project/fake-news-detector/","section":"project","summary":"A fake-news detector based on BERT Language model.","tags":["NLP"],"title":"Fake News Detector","type":"project"},{"authors":[],"categories":[],"content":"","date":1570130340,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570143917,"objectID":"873185202bf43ae885bc155c32162dad","permalink":"https://vjg28.github.io/project/lunar-lander-rl/","publishdate":"2019-10-04T00:49:00+05:30","relpermalink":"/project/lunar-lander-rl/","section":"project","summary":"","tags":["Reinforcement Learning"],"title":"Lunar Lander using Policy Gradient Algorithms","type":"project"},{"authors":[],"categories":[],"content":"","date":1570056061,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570143917,"objectID":"fedd893e0339d705987565c074ceb6e2","permalink":"https://vjg28.github.io/project/arle/","publishdate":"2019-10-03T04:11:01+05:30","relpermalink":"/project/arle/","section":"project","summary":"An effort to make an automated robot capable of mananging (picking and placing) books in the institute library. ","tags":["Robotics"],"title":"ARLE- Autonomous Robot for Library Enhancement","type":"project"},{"authors":[],"categories":[],"content":"","date":1570055058,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570143917,"objectID":"f5c347a58391a0caed7f9c8c69f5ac01","permalink":"https://vjg28.github.io/project/robotic-palm/","publishdate":"2019-10-03T03:54:18+05:30","relpermalink":"/project/robotic-palm/","section":"project","summary":"Robotic Palm control using hand-pose estimation algorithms on live video feed ","tags":["Robotics"],"title":"Robotic Palm Control","type":"project"},{"authors":["Varshil Gandhi"],"categories":[],"content":" Inverse Reinforcement Learning: The core concept of reinforcement learning in Markov decision process revolves around the definition of reward values. The agent observes a state, proceed with an action and the environment provides the reward and the next state.\n   Agent-Environment interaction   However, there are numerous cases where designing reward functions are complicated or \u0026lsquo;Mission Impossible\u0026rsquo;. For example, we want to train an RL agent to drive a car autonomously. The reward function can be hand-engineered, but creating such functions is often time consuming and sub-optimal. Given such reward functions, the agent may not be able to train for the given task.\nIn numerous scenarios like the one mentioned above, the human knows the optimal way to perform the task (Expert policy) but do not understand the underlying reward function behind the task. So, we instead think of ways to use expert human policies instead of reward functions for an agent.\n The simplest of all is just copying the expert behaviour. Given expert trajectories, we design the learning process from a supervised learning perspective: Learn direct mapping from state to actions. This method is known as Behavior Cloning.\n Instead of naive copying, can we recreate the reward function of the environment given expert behaviours? Yes. This approach of imitating an expert through learning the reward functions is known as Inverse Reinforcement Learning.\n     RL vs Inverse RL   The first IRL algorithm (known as Linear Inverse RL) was published in a paper Algorithms for IRL (Ng \u0026amp; Russel 2000) in which they proposed an iterative algorithm to extract the reward function given optimal/expert behaviour policy for obtaining the goal in that particular environment. We will discuss in short what are the significant problems that Inverse RL algorithms face. But, notations first.\nNotations:  Markov decision process : $\u0026lt; S, А, P_{sa}, γ, R \u0026gt;$  Where $S$ = finite/infinite set of states $A$ = action set $P_{sa}$ = transition probability matrix γ = discount factor R = reward function  $π : S➝A $ is th policy function. $ V^{\\pi}(s_t)=E[R(s_t)+R(s_{t+1})] $ Value function $ Q^π(s, a) = R(s) + γ * E_{s\u0026rsquo; \\sim P_{sa}}[ V^π(s’) ] $ Action value function $V^*(s)$ : optimal value function $Q^*(s, a)$ : optimal state value function  The problem is to find a reward function for an environment given optimal behavior/policy. Now, given certain basic conditions, we can easily prove the theorm mentioned below.\nTheorem 1: Let $〈S, А, { Pa }, γ〉$ is known. Then, $π(s)≡a1$ is optimal if and only if for all $a∈ A-a1$ , the reward satisfies the equation $$ (P_{a1} - P_a)(I - γ P_{a1})^{-1} R ≽ 0 $$\nSee the problem in the above equation?\nIt\u0026rsquo;s the degeneracy of reward functions. Each policy has a large set of reward function for which the policy is optimal. The set even includes $R=0 ∀s∈S$. There are even more solutions other than these trivial ones. We do not want these trivial reward functions, as the agent will spend its entire lifetime iterating with 0 reward function and will not learn a thing. So how can we choose reward function from such a vast set of solutions?\nHeuristics : We use some specific heuristics.\n The natural way is to choose a reward function such that deviating from the policy on any single step must be as costly as possible. This intuition is represented in mathematical terms, as mentioned below. $$ ∑_{s∈S}( Q^π(s,a_1) - max_{a∈A-a_1}Q^π(s,a) ) $$ If the above condition does give degenerate results, the second condition is to add a penalty coefficient, $-λ || R ||_1$ . It will make sure that the reward function remains zero in most places and non-zero at only some states with high profit.  The net reward search equation becomes: $$ Maximize ∑_i min_{π∈丌} ~ { p( V^{π*}(s’) - V^{π_i}(s’) ) } $$ $$s.t ~~ |α_i| ≤ 1, ~~~~ i =1,2,..d $$ $$ p(x)=x~when~x\u0026gt;0, ~else ~penalty*x $$ which can be solved with linear programming methods if reward functions are assumed to be linear.\nThe paper explained three possible algorithms varying only w.r.t the underlying conditions.\n Expert policy function $\\pi(s)$ is known, state space $S$ is finite, and transition probabilities $P_{sa}$ are known. (Too ideal case) Expert policy function $\\pi(s)$ is known, state space $S$ is infinite, and transition probabilities $P_{sa}$ are known. (Too ideal case) m expert demonstrations/ trajectories are known. Transition probabilities are not known. (The realistic case)  Algorithm flowchart:    LIRL algorithm flowchart   Experiments: I used the MountainCar-v0 environment of OpenAI gym for training and testing the algorithm.\n Expert Agent: Used Q-learning algorithm with linear approximators to train the agent for generating expert policy.(This approach is used in numerous imitation learning problems as it becomes difficult for humans to generate expert policies in simulated environments as Mountain Car.) Specifics: Detailed specs can be found in the slides.  Refer to the github repository for code and more details.\n    Learned Reward function   This was solely for educational purposes, so I didn\u0026rsquo;t try to exactly match the results of the paper. Mentioned in the slides are the differences and similarities of the experiment with the paper. Do let me know by mail or create an issue on github if you find any discrepancy in the code or any other issues.\n","date":1570053584,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570143917,"objectID":"b043836f6c038a11da638a2a09972daf","permalink":"https://vjg28.github.io/project/linear-inverse-rl/","publishdate":"2019-10-03T03:29:44+05:30","relpermalink":"/project/linear-inverse-rl/","section":"project","summary":"Implementation of LIRL algorithms on Mountain Car Environment.","tags":["Reinforcement Learning"],"title":"Linear Inverse RL Algorithms","type":"project"},{"authors":[],"categories":[],"content":"","date":1569958522,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570143917,"objectID":"a64f562217a4ff7acfb07e317091593c","permalink":"https://vjg28.github.io/project/huffman-coding-arm/","publishdate":"2019-10-02T01:05:22+05:30","relpermalink":"/project/huffman-coding-arm/","section":"project","summary":"","tags":["Others"],"title":"Huffman Coding in ARM-v8","type":"project"},{"authors":null,"categories":null,"content":"Add your content here\u0026hellip;\n","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570143917,"objectID":"fd36605688ef45e10dc233c860158012","permalink":"https://vjg28.github.io/cv/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/cv/","section":"","summary":"Here we describe how to add a page to your site.","tags":null,"title":"An example title","type":"page"}]