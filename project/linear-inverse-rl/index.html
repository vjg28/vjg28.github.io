<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Varshil Gandhi">

  
  
  
    
  
  <meta name="description" content="Implementation of LIRL algorithms on Mountain Car Environment.">

  
  <link rel="alternate" hreflang="en-us" href="https://vjg28.github.io/project/linear-inverse-rl/">

  


  
  
  
  <meta name="theme-color" content="#328cc1">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.543b230cb06cd57f9f44fc76fb511a19.css">

  

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128562026-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-128562026-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://vjg28.github.io/project/linear-inverse-rl/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@varshilgandhi28">
  <meta property="twitter:creator" content="@varshilgandhi28">
  
  <meta property="og:site_name" content="Varshil Gandhi">
  <meta property="og:url" content="https://vjg28.github.io/project/linear-inverse-rl/">
  <meta property="og:title" content="Linear Inverse RL Algorithms | Varshil Gandhi">
  <meta property="og:description" content="Implementation of LIRL algorithms on Mountain Car Environment."><meta property="og:image" content="https://vjg28.github.io/project/linear-inverse-rl/featured.png">
  <meta property="twitter:image" content="https://vjg28.github.io/project/linear-inverse-rl/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-10-03T03:29:44&#43;05:30">
    
    <meta property="article:modified_time" content="2019-10-04T04:35:17&#43;05:30">
  

  


    











<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://vjg28.github.io/project/linear-inverse-rl/"
  },
  "headline": "Linear Inverse RL Algorithms",
  
  "image": [
    "https://vjg28.github.io/project/linear-inverse-rl/featured.png"
  ],
  
  "datePublished": "2019-10-03T03:29:44+05:30",
  "dateModified": "2019-10-04T04:35:17+05:30",
  
  "author": {
    "@type": "Person",
    "name": "Varshil Gandhi"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Varshil Gandhi",
    "logo": {
      "@type": "ImageObject",
      "url": "https://vjg28.github.io/img/icon-512.png"
    }
  },
  "description": "Implementation of LIRL algorithms on Mountain Car Environment."
}
</script>

  

  


  


  





  <title>Linear Inverse RL Algorithms | Varshil Gandhi</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Varshil Gandhi</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article article-project">

  




















  
  


<div class="article-container pt-3">
  <h1>Linear Inverse RL Algorithms</h1>

  

  



<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/varshil-gandhi/">Varshil Gandhi</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Oct 4, 2019
  </span>
  

  

  

  
  
  
  <span class="middot-divider"></span>
  <a href="/project/linear-inverse-rl/#disqus_thread"></a>
  

  
  

  
    

  

</div>

  













<div class="btn-links mb-3">
  
  








  












  
  <a class="btn btn-outline-primary my-1 mr-1" href="/slides/irl-slides/" target="_blank">
    Slides
  </a>
  





  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1" href="https://github.com/vjg28/Linear-Inverse-RL-algorithms" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>
    Github
  </a>


</div>


</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 592px; max-height: 388px;">
  <div style="position: relative">
    <img src="/project/linear-inverse-rl/featured.png" alt="" class="featured-image">
    <span class="article-header-caption">Mountain-Car Environment Credits:OpenAI</span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      

<h2 id="inverse-reinforcement-learning">Inverse Reinforcement Learning:</h2>

<p>The core concept of reinforcement learning in Markov decision process revolves around the definition of <strong>reward values</strong>. The agent observes a state, proceed with an action and the environment provides the reward and the next state.</p>














<figure>


  <a data-fancybox="" href="RL_env.png" data-caption="Agent-Environment interaction">
<img src="RL_env.png" alt="" ></a>


  
  
  <figcaption>
    Agent-Environment interaction
  </figcaption>


</figure>


<p>However, there are numerous cases where designing reward functions are complicated or &lsquo;Mission Impossible&rsquo;. For example, we want to train an RL agent to drive a car autonomously. The reward function can be hand-engineered, but creating such functions is often time consuming and sub-optimal. Given such reward functions, the agent may not be able to train for the given task.</p>

<p>In numerous scenarios like the one mentioned above, the human knows the optimal way to perform the task (Expert policy) but do not understand the underlying reward function behind the task. So, we instead think of ways to use expert human policies instead of reward functions for an agent.</p>

<ol>
<li><p>The simplest of all is just copying the expert behaviour. Given expert trajectories, we design the learning process from a supervised learning perspective: Learn direct mapping from state to actions. This method is known as <strong>Behavior Cloning</strong>.</p></li>

<li><p>Instead of naive copying, can we recreate the reward function of the environment given expert behaviours? Yes. This approach of imitating an expert through learning the reward functions is known as <strong>Inverse Reinforcement Learning</strong>.</p></li>
</ol>














<figure>


  <a data-fancybox="" href="irl.png" data-caption="RL vs Inverse RL">
<img src="irl.png" alt="" ></a>


  
  
  <figcaption>
    <strong>RL vs Inverse RL</strong>
  </figcaption>


</figure>


<p>The first IRL algorithm (known as Linear Inverse RL) was published in a paper <a href="https://ai.stanford.edu/~ang/papers/icml00-irl.pdf" target="_blank">Algorithms for IRL</a> (Ng &amp; Russel 2000) in which they proposed an iterative algorithm to extract the reward function given optimal/expert behaviour policy for obtaining the goal in that particular environment. We will discuss in short what are the significant problems that Inverse RL algorithms face. But, notations first.</p>

<h3 id="notations">Notations:</h3>

<ol>
<li>Markov decision process : $&lt; S, А, P_{sa}, γ, R &gt;$

<ul>
<li>Where $S$ = finite/infinite set of states</li>
<li>$A$ = action set</li>
<li>$P_{sa}$ = transition probability matrix</li>
<li>γ = discount factor</li>
<li>R = reward function</li>
</ul></li>
<li>$π : S➝A $ is th policy function.</li>
<li>$ V^{\pi}(s_t)=E[R(s_t)+R(s_{t+1})] $ Value function</li>
<li>$ Q^π(s, a) = R(s) + γ * E_{s&rsquo; \sim  P_{sa}}[ V^π(s’) ] $ Action value function</li>
<li>$V^*(s)$ : optimal value function</li>
<li>$Q^*(s, a)$ : optimal state value function</li>
</ol>

<p>The problem is to find a reward function for an environment given optimal behavior/policy. Now, given certain basic conditions, we can easily prove the theorm mentioned below.</p>

<p><strong>Theorem 1</strong>: Let $〈S, А, { Pa }, γ〉$ is known. Then, $π(s)≡a1$ is optimal if and only if for all $a∈ A-a1$ , the reward satisfies the equation
$$ (P_{a1} - P_a)(I - γ P_{a1})^{-1} R ≽ 0 $$</p>

<p>See the problem in the above equation?</p>

<p>It&rsquo;s the <strong>degeneracy of reward functions</strong>. Each policy has a large set of reward function for which the policy is optimal. The set even includes $R=0 ∀s∈S$. There are even more solutions other than these trivial ones. We do not want these trivial reward functions, as the agent will spend its entire lifetime iterating with 0 reward function and will not learn a thing. So how can we choose reward function from such a vast set of solutions?</p>

<h3 id="heuristics">Heuristics :</h3>

<p>We use some specific heuristics.</p>

<ol>
<li>The natural way is to choose a reward function such that deviating from the policy on any single step must be as costly as possible. This intuition is represented in mathematical terms, as mentioned below. $$ ∑_{s∈S}( Q^π(s,a_1) -  max_{a∈A-a_1}Q^π(s,a) ) $$</li>
<li>If the above condition does give degenerate results, the second condition is to add a penalty coefficient,  $-λ || R ||_1$ . It will make sure that the reward function remains zero in most places and non-zero at only some states with high profit.</li>
</ol>

<p>The net reward search equation becomes:
$$
Maximize ∑_i min_{π∈丌} ~ { p( V^{π*}(s’) - V^{π_i}(s’) ) } $$
    $$s.t ~~ |α_i| ≤ 1,  ~~~~ i =1,2,..d $$
    $$ p(x)=x~when~x&gt;0, ~else ~penalty*x $$
which can be solved with linear programming methods if reward functions are assumed to be linear.</p>

<p>The paper explained three possible algorithms varying only w.r.t the underlying conditions.</p>

<ol>
<li>Expert policy function $\pi(s)$ is known, state space $S$ is finite, and transition probabilities $P_{sa}$ are known. (<strong>Too ideal case</strong>)</li>
<li>Expert policy function $\pi(s)$ is known, state space $S$ is infinite, and transition probabilities $P_{sa}$ are known. (<strong>Too ideal case</strong>)</li>
<li>m expert demonstrations/ trajectories are known. Transition probabilities are not known. (<strong>The realistic case</strong>)</li>
</ol>

<h3 id="algorithm-flowchart">Algorithm flowchart:</h3>














<figure>


  <a data-fancybox="" href="IRL_algo_layout.png" data-caption="LIRL algorithm flowchart">
<img src="IRL_algo_layout.png" alt="" ></a>


  
  
  <figcaption>
    LIRL algorithm flowchart
  </figcaption>


</figure>


<h3 id="experiments">Experiments:</h3>

<p>I used the MountainCar-v0 environment of OpenAI gym for training and testing the algorithm.</p>

<ul>
<li><strong>Expert Agent</strong>: Used Q-learning algorithm with linear approximators to train the agent for generating expert policy.(This approach is used in numerous imitation learning problems as it becomes difficult for humans to generate expert policies in simulated environments as Mountain Car.)</li>
<li><strong>Specifics</strong>: Detailed specs can be found in the <a href="https://vjg28.github.io/slides/irl-slides/" target="_blank">slides</a>.</li>
</ul>

<p>Refer to the <a href="https://github.com/vjg28/Linear-Inverse-RL-algorithms" target="_blank">github repository</a> for code and more details.</p>

<p>












<figure>


  <a data-fancybox="" href="learnt_reward.png" data-caption="Learned Reward function">
<img src="learnt_reward.png" alt="" ></a>


  
  
  <figcaption>
    Learned Reward function
  </figcaption>


</figure>

This was solely for educational purposes, so I didn&rsquo;t try to exactly match the results of the paper. Mentioned in the slides are the differences and similarities of the experiment with the paper. Do let me know by mail or create an issue on github if you find any discrepancy in the code or any other issues.</p>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/reinforcement-learning/">Reinforcement Learning</a>
  
</div>


    








  
  
    
  
  






  
  
  
  
  <div class="media author-card">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/varshil-gandhi/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>




    
<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "eventhorizon-28" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>



    
    

    
    
    

    
    
    

    
    
    

  </div>
</article>



      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://eventhorizon-28.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.6cbefe3f0755301b86cd38317ded9f54.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
